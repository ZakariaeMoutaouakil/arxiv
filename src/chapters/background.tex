\section{Background \& Related Work}\label{sec:background-&-related-work}

\subsection{Notation}\label{subsec:notation}
Let $\mathcal{X}$ be a subset of the input space $\mathbb{R}^d$.
We identify the set of labels $\mathcal{Y}$ with the set of integers $\{1, \ldots, m\}$, where $m$ is the number of classes.

If $x$ is a data point in $\mathcal{X}$ and $y$ is its true label in $\mathcal{Y}$, then the prediction of a typical neural network classifier $F_{out}: \mathcal{X} \rightarrow \mathcal{Y}$ can be decomposed as $F_{out}(x)=\arg\max_{k \in \mathcal{Y}}s_k\circ f(x)$ where $g: \mathcal{X} \rightarrow \mathbb{R}^d$ is a subclassifier that outputs logits
and $s: \mathbb{R}^d \rightarrow \Delta^{d-1}$ is the normalizing last layer that projects onto the $(d-1)$-dimensional probability simplex defined as:
\[
    \Delta^{d-1} \coloneqq \left\{p \in \mathbb{R}^d : \, p_i \geq 0, \,\sum_{i=1}^d p_i = 1 \right\}.
\]

We denote by $F\coloneqq s\circ f$ the classifier that outputs a probability distribution over the $m$ classes.
We call $F$ the \textbf{soft classifier}, and $F_{out}$ the \textbf{hard classifier}.

The simplex maps that are most commonly found in the literature are the softmax, the hardmax that is defined as the projection that puts all mass in the index of the maximum component (with ties broken arbitrarily), and the sparsemax that is defined as the Hilbert projection onto the convex closed set which is the $d-1$-dimensional probability simplex.

We will call the case where we use the hardmax as a simplex map the \textbf{discrete case} and the case where we use a continuous simplex projection (like the softmax or the sparsemax) the \textbf{continuous case}.

\subsection{Randomized Smoothing}\label{subsec:randomized-smoothing}
Randomized smoothing is a technique designed to enhance the robustness of classifiers against small perturbations or adversarial attacks that stems from a simple intuition: if a classifier's decision is stable under small random perturbations of the input, it's likely to be more robust overall (this intuition will be made more rigorous using the concept of robustness radius).

Following the same conventions as in the previous section, we define the smoothed classifier $\hat{F}: \mathcal{X} \rightarrow \Delta^{d-1}$ using randomized smoothing as implementing a majority vote over predictions of the soft classifier $F$ with noisy versions of the input:

\begin{align*}
    \hat{F}(x) \coloneqq \mathbb{E}_{\epsilon \sim \mathcal{N}(0, \sigma^2 I)} [F(x + \epsilon)],
\end{align*}

where $\epsilon \sim \mathcal{N}(0, \sigma^2 I)$ represents Gaussian noise with mean $0$ and covariance matrix $\sigma^2 I$.
The final smoothed classifier is given by $\hat{F}_{out}(x) = \arg\max_{k \in \mathcal{Y}} \hat{F}_k(x)$.
We can reinterpret this expectation as convolving $F$ with the Gaussian kernel $p_\sigma$; $\hat{F} = F * p_\sigma$, where
\begin{align*}
    p_\sigma(z) \coloneqq \frac{1}{(2\pi\sigma^2)^{d/2}} \exp\left(-\frac{\|z\|^2}{2\sigma^2}\right).
\end{align*}

\subsection{Robustness Radius}\label{subsec:robustness-radius}

For a given classifier $F: \mathcal{X} \rightarrow \Delta^{d-1}$, we can quantify its decision confidence for a specific input $x$ using a metric called the certified radius, denoted as $R(F,x)$.
This radius represents the maximum allowable perturbation $\epsilon$ that can be applied to the input $x$ while ensuring the classifier's output remains consistent with the true label $y$.

The certified radius serves as an indicator of a classifier's resilience to input disturbances.
A larger value of $R(F,x)$ suggests greater robustness against perturbations for that particular input.

In the context of randomized smoothing with Gaussian noise, the size of the perturbation is usually measured using the $\ell_2$-norm, therefore we can formally define the certified radius as:

\begin{align*}
    R(F,x) \coloneqq
    \begin{cases}
        0 & \text{if } F_{out}(x) \neq y, \\[2ex]
        \min \left\{ \epsilon > 0 \,|\, \exists \tau \in B_2(0,\epsilon),\, F_{out}(x + \tau) \neq y \right\} & \text{otherwise,}
    \end{cases}
\end{align*}

where $B_2(0,\epsilon)$ represents the $\ell_2$-ball centered at the origin with radius $\epsilon$.

\subsection{Certified Radius and Lipschitz Constant}\label{subsec:certified-radius-and-lipschitz-constant}

The concept of Lipschitz continuity provides an intuitive framework for understanding classifier robustness and its relation to the certified radius.

A soft classifier $F: \mathcal{X} \rightarrow \Delta^{d-1}$ is Lipschitz continuous if there exists a constant $L \geq 0$ such that for all $x_1, x_2 \in \mathcal{X}$:
\begin{align*}
    \lVert F(x_1)- F(x_2) \rVert \leq L \cdot \lVert  x_1- x_2\rVert.
\end{align*}
Lipschitz continuity essentially bounds the maximum change in the output for a given change in the input, which means that a smaller Lipschitz constant indicates that the classifier's output is less sensitive to input perturbations.

The following proposition highlights the relationship between Lipschitz continuity and a quantity named the prediction margin $M(f,x)$ (defined below) which quantifies how far the input is from the decision boundary of the classifier.
A positive margin indicates correct classification, with larger values suggesting higher confidence, while a negative margin indicates misclassification:
\begin{equation}
    M(F,x) \coloneqq F(x)_{y} - \max_{i \neq y}\{F(x)_i\}.\label{eq:prediction-margin}
\end{equation}
\begin{proposition}
    If the classifier $F$ is Lipschitz continuous with Lipschitz constant $L(F)$ and if $M(F,x)>\sqrt{2}L(F)r$, then for any adversarial example $x+\epsilon$ such that $\lVert \epsilon \rVert\leq r$, $M(F,x+\epsilon)>0$.
\end{proposition}

As a corollary of the proposition above, we can induce a certified radius that is proportional to the prediction margin:
\[
    R(F,x) \geq \frac{M(F,x)}{\sqrt{2}L(F)}
\]

Going back to randomized smoothing, we can demonstrate easily from the definition of the smoothed classifier $\hat{F}$ that its Lipschitz constant is less than $L(F)$.
Moreover, if the simplex map $s$ is $1$-Lipschitz (like in the case of softmax or sparsemax), then $L(F)\leq L(f)$.
As a result, we can write our first certified radius for a smoothed classifier as follows:
\begin{equation}
    R_1(\hat{F},x) \coloneqq \frac{M(\hat{F},x)}{\sqrt{2}L(f)}\label{eq:first-radius}
\end{equation}

\subsection{Certified Radius and Noise Magnitude}\label{subsec:certified-radius-and-noise-magnitude}
In the discrete case, Cohen et al.
proved a model-agnostic certifiably tight radius for randomized smoothing, meaning meaning there exist adversarial examples at distances arbitrarily close to the ball centered at $x$ with their radius.
The method scales to large networks and high-dimensional data, as it only requires evaluating the base classifier on noisy samples.

Formally, their certified radius is defined as:
\begin{equation}
    R_2(\hat{F},x) \coloneqq \frac{\sigma}{2}M(\Phi^{-1}\circ\hat{F},x)=\frac{\sigma}{2}\left(\Phi^{-1}(\hat{F}(x)_{y})-\Phi^{-1}(\max_{i \neq y}\{ (\hat{F}(x)_i) \})\right),\label{eq:second-radius}
\end{equation}
where $\Phi$ is the Gaussian cumulative distribution function.
The appearance of the Gaussian CDF ($\Phi$) in this formula is tied to the blurring effect of the decision boundary induced by the addition of Gaussian noise to the input.

This guarantee is tight because it considers the worst-case scenario where all the probability mass of the runner-up class is concentrated at the point closest to the decision boundary.
It's worth noting that the noise magnitude $\sigma$ plays a crucial role in both cases.
Larger $\sigma$ values tend to increase the certified radius of the smoothed classifier, potentially improving robustness.
However, this comes at the cost of potentially reduced accuracy on clean (unperturbed) inputs, as the noise may cause misclassifications even in the absence of adversarial perturbations, embodying a fundamental trade-off in randomized smoothing between robustness and clean accuracy.