\section{Problem Formulation}\label{sec:problem-formulation}

\subsection{Monte Carlo Simulation}\label{subsec:monte-carlo-simulation}

Both certified radii~\eqref{eq:first-radius} and~\eqref{eq:second-radius} require evaluating the output of the smoothed classifier $\hat{F}$.
The evaluation of a smoothed classifier presents a unique challenge due to the intractable nature of the integral involved in its definition.
To overcome this limitation, the classical approach is to use Monte Carlo methods, which provide a practical and efficient means of approximating the output of the smoothed classifier, capable of scaling to large networks and high-dimensional data.

The Monte Carlo approach relies on the law of large numbers to approximate the true expectation.
The process begins by generating a large number of independent Gaussian noise samples, typically in the order of thousands or tens of thousands.
These noise samples are then added to the input $x$, creating a set of perturbed inputs.
The base classifier $F$ is then evaluated on each of these perturbed inputs, producing a collection of outputs.
The key insight of the Monte Carlo method is that the empirical distribution of these outputs converges to the true distribution of the smoothed classifier's output as the number of samples increases.

The exact procedure differs slightly depending on whether we are dealing with a discrete classifier (outputting class labels directly) or a continuous classifier (outputting logits or probabilities).

\subsubsection{Discrete Case}\label{subsubsec:discrete-case-monte-carlo-simulation}
In the discrete case, our base classifier $F$ outputs one-hot dimensional vectors that we can assimilate to class labels.
To approximate the output of the smoothed classifier $\Hat{F}$, we generate a vector of counts $X = (X_1, \ldots, X_m)$ through a sampling process.
This process begins by generating $n$ independent noise samples from a Gaussian distribution $\mathcal{N}(0, \sigma^2 I)$.
We then evaluate the base classifier on each of these noisy inputs, producing a set of class labels.
Finally, we count the occurrences of each class to form our vector $X$.
The procedure is illustrated in Algorithm~\ref{alg:vector-of-counts}.
\begin{algorithm}[htbp]
    \DontPrintSemicolon % Removes semicolons at the end of lines
    \KwIn{The base classifier $F$, the number of samples $n$, the standard deviation $\sigma$ and an input $x$}
    \KwOut{The vector of counts $X = (X_1, \ldots, X_m)$}
    Initialize $X = (0, \ldots, 0)$\;
    \For{$i = 1$ \KwTo $n$}{
        Generate noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2 I)$\;
        $\tilde{x}_i = x + \epsilon_i$\;
        $y_i = F(\tilde{x}_i)$\;
        Increment $X_{y_i}$ by 1\;
    }
    \KwRet{$X$}
    \caption{Sampling in the Discrete Case}\label{alg:vector-of-counts}
\end{algorithm}

This vector of counts $X$ can be interpreted as a sample from a multinomial distribution with parameters $n$ and $p = (p_1, \ldots, p_m)$, where $p_k = \mathbb{P}(f(x + \epsilon) = k)$.
The probability vector $p$ is precisely what we aim to approximate, as it represents the output of the smoothed classifier.
To obtain a concrete prediction, we simply select the class with the highest count: $\hat{F}_{out}(x) = \argmax_{k \in \{1, \ldots, m\}} X_k$.
This approach effectively approximates the argmax operation over the class probabilities in the definition of the smoothed classifier.

The law of large numbers ensures that as $n \rightarrow \infty$, the empirical probabilities $X_k / n$ converge to the true probabilities $p_k$, providing a consistent estimator of the smoothed classifier's output.
Moreover, the multinomial nature of $X$ allows us to construct confidence intervals for the true probabilities $p_k$ using methods such as the Clopper-Pearson interval.
These confidence intervals play a crucial role in certifying the robustness of the smoothed classifier, as they allow us to bound the probabilities of the top class and the runner-up with high confidence.

\subsubsection{Continuous Case}\label{subsubsec:continuous-case-monte-carlo-simulation}
In the continuous case, our base classifier $F: \mathbb{R}^d \rightarrow \Delta^{m-1}$ outputs a vector of probabilities for each class.
Here, instead of a vector of counts, we produce a matrix $X \in \mathbb{R}^{n \times m}$ where each row represents the output of the base classifier for a single noisy sample.
The process begins similarly to the discrete case, with the generation of $n$ independent noise samples.
We then evaluate the base classifier on each noisy input, but instead of counting class labels, we store the full output vectors as rows of our matrix $X$.
Algorithm~\ref{alg:probability-matrix} explains the sampling process.
\begin{algorithm}[htbp]
    \DontPrintSemicolon
    \KwIn{The base classifier $F: \mathbb{R}^d \rightarrow \Delta^{m-1}$, the number of samples $n$, the standard deviation $\sigma$ and an input $x$}
    \KwOut{The grid matrix $X \in \mathbb{R}^{n \times m}$}
    Initialize $X$ as an empty $n \times m$ matrix\;
    \For{$i = 1$ \KwTo $n$}{
        Generate noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2 I)$\;
        $\tilde{x}_i = x + \epsilon_i$\;
        $y_i = F(\tilde{x}_i)$\;
        Set the $i$-th row of $X$ to $y_i$\;
    }
    \KwRet{$X$}
    \caption{Sampling in the Continuous Case}\label{alg:probability-matrix}
\end{algorithm}

Each column of $X$ represents the varying confidence scores for a particular class label across the noisy samples.
The column-wise means $\hat{p}_j = \frac{1}{n} \sum_{i=1}^n X_i^j$ provide an estimate of the expected confidence for each class under the noise distribution, effectively approximating the output of the smoothed classifier.
This approach not only captures the central tendency of the classifier's behavior but also allows us to compute confidence intervals that account for the fluctuations in these scores.
The final prediction is then determined by the class with the highest mean confidence: $\hat{F}_{out}(x) = \argmax_{k \in \{1, \ldots, m\}} \hat{p}_k$.
By considering both the mean and the variability of these confidence scores, we can obtain a more nuanced understanding of the classifier's behavior under noise, which is crucial for both prediction and robustness certification.

In this case, the central limit theorem ensures that $\hat{p}$ converges to a multivariate normal distribution as $n \rightarrow \infty$, centered at the true expected logits.
This allows us to construct confidence intervals for the output of the smoothed classifier.
The continuous case offers some advantages in terms of statistical efficiency, as it retains more information from each evaluation of the base classifier.
However, it may be more computationally intensive, especially for large $m$, as it requires storing and manipulating the full matrix $X$ rather than just a vector of counts.

In both the discrete and continuous cases, the accuracy of the Monte Carlo approximation improves with the number of samples $n$, allowing for a trade-off between computational cost and precision.
Larger $n$ provides tighter confidence intervals and more reliable predictions, but at the cost of increased computation time and memory usage.
The choice between the discrete and continuous approaches may depend on the specific base classifier and the computational resources available.
The discrete approach is often simpler to implement and may be more memory-efficient, especially for large $m$.
The continuous approach, while potentially more statistically efficient, requires more storage and computation, particularly when dealing with logits.

In practice, the choice of $n$ and $\sigma$ involves balancing multiple objectives: prediction accuracy, robustness guarantees, computational efficiency, and clean accuracy.
These parameters may need to be tuned based on the specific requirements of the application and the characteristics of the dataset and base classifier.

\subsection{Monte Carlo Approximation and Conservative Confidence Intervals}\label{subsec:monte-carlo-approximation-and-conservative-confidence-intervals}

While the Monte Carlo methods offers a practical means of approximating the output of a smoothed classifier, it introduces an element of randomness into the evaluation process, a factor that must be carefully considered, especially when certifying the robustness of these classifiers, since the certified radius represents a guarantee of the classifier's stability under adversarial perturbations, and any uncertainty in its computation could potentially undermine this guarantee.

This transformation of the smoothed classifier from a deterministic model to a stochastic one allows us to employ probabilistic reasoning about its outputs.
We can conceptualize the outputs as random vectors, which enables the application of statistical analysis to assess and quantify the variability and confidence of the predictions.
This stochastic framework necessitates a shift in how we evaluate the performance and reliability of the classifier, requiring metrics that can capture and articulate the probabilistic nature of the outputs.

The biggest risk induced by the randomness in our Monte Carlo approximation is that our empirical estimate might overstate the true robustness of the classifier.
If we were to naively use our empirical estimates without accounting for this risk, we might end up with an overly optimistic assessment of the classifier's robustness.
Such an overestimation could have serious consequences in security-critical applications, where reliance on an inflated robustness guarantee could lead to vulnerabilities.

To address this risk and maintain the integrity of our robustness guarantees, the use of confidence intervals is key.
Specifically, we employ a one-sided lower confidence bound on our estimate of the certified radius.
In the context of robustness certification, we are primarily concerned with avoiding overestimation of the classifier's capabilities.
An underestimation, while potentially leading to a smaller certified radius, does not compromise the validity of our robustness guarantee.
It merely results in a more conservative certification, which is preferable to an optimistic one that might not hold in practice.

The use of a confidence interval highlights two important trade-offs.
The first trade-off that we touched on in the previous section is the influence of the number of Monte Carlo samples on the width of the confidence interval, and consequently the conservatism of our robustness estimate.
Increasing the number of samples narrows the confidence interval, potentially leading to tighter robustness bounds.
However, this comes at the cost of increased computational overhead.
In practice, the choice of sample size often involves balancing the desire for tight bounds with computational constraints.

Another factor to consider is the choice of confidence level for these intervals.
A higher confidence level (e.g., 99\% vs 95\%) provides stronger statistical guarantees but results in wider intervals and thus smaller certified radii.
Again, this represents a trade-off between the strength of the probabilistic guarantee and the size of the certified radius.

\subsection{Practical Calculation of Confidence Interval}\label{subsec:practical-calculation-of-confidence-interval}

When trying to estimate both certified radii, the heavy-lifting part of the calculation involves estimating the (prediction) margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$.

The classical statistical approach to estimate the margin is to combine an exact one-sided interval confidence interval with Bonferroni correction and is outlined in Algorithm~\ref{alg:bonferroni}.
The use of exact intervals, rather than approximate ones, is crucial in this context.
Exact intervals guarantee that the true probability is contained within the interval with at least the specified confidence level, regardless of the sample size or the true underlying probability.
This property is particularly important when certifying robustness, as we need to ensure that our guarantee holds with high probability.
Approximate intervals, such as those based on normal approximations, may underestimate the uncertainty for extreme probabilities or small sample sizes, potentially leading to overly optimistic robustness certificates.
The exact interval is then used to get a lower bound to the probability of the predicted class while simultaneously applying an upper bound to the probability of all other classes to yield a conservative estimate of the classifier's robustness.

To account for the fact that we are estimating multiple probabilities simultaneously, we apply the Bonferroni correction.
This correction adjusts the significance level for each individual confidence interval to ensure that the overall probability of any interval not containing the true value remains below our chosen significance level.

\begin{algorithm}[htbp]
    \DontPrintSemicolon % Removes semicolons at the end of lines
    \KwIn{A vector of counts or a grid matrix $X$, and a routine \texttt{getConfidenceBound()} that can compute a lower/upper confidence bound at a given confidence level $1-\alpha$ (the arguments of the \texttt{getConfidenceBound()} routine are omitted for conciseness and generalization)}
    \KwOut{The estimated margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$ with confidence level $1-\alpha$}

    Computer a lower confidence bound $\underline{\Hat{F}(x)}_{y}$ on the output of the correct class $\Hat{F}(x)_{y}$ using the \texttt{getConfidenceBound()} routine at level $\frac{\alpha}{m}$\;
    \For{all other classes $j$ other than $y$}{
        Compute an upper confidence bound $\overline{\Hat{F}(x)}_{j}$ on the output of the class $j$ using the \texttt{getConfidenceBound()} routine at level $\frac{\alpha}{m}$\;
    }
    Plug in the lower and upper confidence bounds to the margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$\;
    \KwRet{$M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$}
    \caption{The Bonferroni approach to estimate the margin}\label{alg:bonferroni}
\end{algorithm}

Intuitively, the application of a lower bound to the highest probability effectively diminishes the perceived confidence of the classifier in its primary prediction.
This deliberate underestimation of confidence creates a more stringent condition for establishing robustness guarantees.
It simulates a scenario where the classifier operates under suboptimal conditions, thereby testing its resilience in a more challenging environment than may actually exist.

Conversely, for the runner-up class, the utilization of an upper bound on its probability artificially enhances the perceived strength of the alternative classification.
This overestimation of the competing class's likelihood presents a more formidable challenge to the primary prediction.
The effect is to reduce the apparent margin between the top two classes, thus imposing a more rigorous criterion for demonstrating the classifier's decisiveness.

The confluence of these two conservative estimates – the understatement of the classifier's confidence in its primary prediction and the overstatement of the potential for an alternative classification – engenders a scenario that approaches a worst-case within the bounds of statistical confidence.
This conservative framework ensures that any subsequent robustness guarantees derived from these estimates will remain valid even under these pessimistic assumptions.
Consequently, this approach provides a highly reliable, albeit potentially overly cautious, assessment of the classifier's robustness.

As a last remark, it is worth explaining why Algorithm~\ref{alg:bonferroni} computes upper confidence bounds for all classes other than the predicted class, rather than just the second-highest class, due to an important nuance in how confidence bounds account for variance.
When we calculate upper confidence bounds, we're not just considering the mean (expected) output of each class, but also the uncertainty or variance associated with that output.
This means that a class with a lower mean output could potentially have a higher upper confidence bound if it has greater variance.
For example, suppose we have three classes: A (predicted class), B, and C. While B might have a higher mean output than C, C could have much higher variance.
When we compute the upper confidence bounds, C's bound might actually exceed B's, despite its lower mean, due to this higher variance.
By considering all non-predicted classes, we ensure we capture the worst-case scenario across all possible competitors to the predicted class, not just the one that appears strongest on average.
This approach provides a more robust and conservative estimate of the classifier's performance, accounting for potential fluctuations and uncertainties in the model's outputs.

\subsubsection{Discrete Case}\label{subsubsec:discrete-case}
In practice, computing this margin exactly as defined can be computationally intensive, especially for classifiers with a large number of classes.
However, a useful simplification can be made without compromising the conservative nature of our robustness estimate.

In the discrete case, where the classifier outputs class labels rather than continuous logits or probabilities, we encounter a fundamentally different scenario that justifies focusing solely on the class with the second-highest count.
This simplification is valid and doesn't compromise the conservative nature of our robustness estimate for several reasons.

First, in a discrete classification scenario, we're dealing with counts of occurrences rather than probability distributions.
Each sample is categorically assigned to one class, and our confidence in a class is directly proportional to the number of times it was chosen.
This discrete nature eliminates the concept of variance as we understand it in continuous distributions.

When we're working with counts, the uncertainty doesn't manifest as variance within each class's score, but rather in the overall distribution of counts across classes.
The class with the highest count is our predicted class, and the class with the second-highest count represents the strongest potential competitor.

In this discrete setup, it's impossible for a class with fewer counts to suddenly ``overtake'' a class with more counts when we calculate confidence bounds.
The ordering of classes by their counts is fixed and doesn't change regardless of how we calculate confidence intervals.
This is in stark contrast to the continuous case, where a class with a lower mean but higher variance could potentially have a higher upper confidence bound.

Moreover, in the discrete case, our confidence bounds are typically calculated using methods designed for count data, such as the binomial proportion confidence interval.
These methods directly relate to the observed counts and don't introduce the possibility of rank swapping that we see in continuous distributions.

The robustness of our classifier in this discrete case is essentially a measure of how convincingly the top class outperforms its nearest competitor.
By focusing on the class with the second-highest count, we're always considering the worst-case scenario among the non-predicted classes.
Any class with fewer counts than the second-highest will invariably have a lower upper confidence bound, making it less of a threat to our robustness estimate.

This simplification to consider only the second-highest class is not just computationally efficient; it's also intuitively aligned with the discrete nature of the problem.
We're essentially asking: ``How certain are we that our top choice is correct, given the performance of its strongest competitor?'' This pairwise comparison captures the essence of our robustness concern without needing to consider classes that performed even more poorly.

The discrete nature of the classification outputs in this scenario fundamentally changes how we approach uncertainty and confidence.
The absence of within-class variance, the fixed nature of the count-based ranking, and the direct relationship between counts and confidence bounds all contribute to making the focus on the second-highest class both valid and sufficient for a conservative robustness estimate.

Formally, if we denote the index of the class with the second-highest score as $j$, we can approximate the margin as:
\begin{align*}
    M(\Hat{F},x) =\Hat{F}(x)_{y} - \Hat{F}(x)_j, \text{ and }M(\Phi^{-1}\circ\Hat{F},x) =\Phi^{-1}(\Hat{F}(x)_{y}) - \Phi^{-1}(\Hat{F}(x)_j).
\end{align*}

Thus, in Algorithm~\ref{alg:bonferroni}, we can replace $m$ with $2$.
In the discrete case, the Clopper-Pearson interval (CP), also known as the exact binomial confidence interval, provides a conservative estimate of the true probability of an event given a number of observed successes in a fixed number of trials.
In our context, we use this method to construct confidence intervals for the probabilities of the top class and the runner-up class.

Concretely, let $n_y$ be the number of times the top class $y$ is observed in $n$ Monte Carlo samples, and $n_j$ be the count for the runner-up class $j$.
We construct a lower bound $\underline{p_y}$ for the probability of class $y$ and an upper bound $\overline{p_j}$ for class $j$ using the Clopper-Pearson method:
\begin{gather*}
    \underline{p_y} = \text{BetaInv}(\alpha/2, n_y, n - n_A + 1)\\
    \overline{p_j} = \text{BetaInv}(1-\alpha/2, n_j + 1, n - n_j)
\end{gather*}

Here, $\text{BetaInv}$ is the inverse of the cumulative distribution function of the Beta distribution, and $\alpha$ is the chosen significance level (typically 0.001 or smaller).

Consequently, we can use these bounds to compute a conservative estimate of the margin:

\begin{equation}
    \Hat{M}(\Hat{F},x)= \underline{p_y} - \overline{p_j}, \quad\Hat{M}(\Phi^{-1}\circ\Hat{F},x) = \Phi^{-1}(\underline{p_y}) - \Phi^{-1}(\overline{p_j})\label{eq:clopper-pearson}
\end{equation}

where $\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.

This conservative margin estimate can then be used to compute the certified radius, ensuring that our robustness guarantee holds with high probability.
The use of these statistical techniques allows us to make strong claims about the robustness of our smoothed classifier, even when working with finite samples in a probabilistic setting.
By consistently underestimating the true margin, we maintain the validity of our robustness certificates at the cost of potentially being overly conservative in some cases.

\subsubsection{Continuous Case}\label{subsubsec:continuous-case}
In the continuous case, to obtain a lower or an upper bound on the mean of a class probability, the corresponding column in the prediction matrix $X$ serves as a sample we can use to estimate the mean with an exact confidence interval.
The classical approach to obtaining confidence intervals involves inverting concentration inequalities which describe how a random variable deviates from its expected value.
These inequalities provide upper bounds on the probability that a random variable differs from its mean by more than a certain amount.
Common examples include Markov's inequality, Chebyshev's inequality, and Hoeffding's inequality.
To derive a confidence interval, we simply invert these concentration inequalities.

This inversion essentially flips the perspective: instead of bounding the probability of deviation, we bound the range of possible values for the parameter of interest.
The key idea is to express the probability statement in terms of the unknown parameter rather than the random variable.
This inversion process typically involves algebraic manipulation of the inequality.
We start with the concentration inequality, which gives us a probabilistic bound on the deviation of our estimator from the true parameter value.
By rearranging this inequality, we can express it in terms of the parameter itself, creating a range of values that likely contains the true parameter with a specified level of confidence.

The resulting confidence interval provides a range of plausible values for the unknown parameter, along with a measure of the uncertainty associated with the estimate.
The width of this interval is influenced by factors such as the sample size, the variability in the data, and the desired level of confidence.

In randomized smoothing, the state-of-the-art approach uses a variance-adaptive concentration inequality to obtain confidence bounds.
Other variance-agnostic approaches have limitations, particularly in scenarios with low variance which are the scenarios that we are interested in, since well-trained classifiers (like neural networks trained using Gaussian data augmentation or adversarial training) have very low variance in their outputs for similar inputs.
For example, Hoeffding's inequality assumes the worst-case scenario for variance, which is $1/4$ for a bounded random variable in $[0,1]$.
This assumption can lead to overly wide confidence intervals when the actual variance is much smaller than this upper bound.

Unlike Hoeffding's, Bernstein's inequality takes into account the actual variance of the random variables.
By incorporating this information, Bernstein's inequality can provide tighter bounds, especially in low-variance scenarios relative to their range.
However, the standard Bernstein inequality still requires knowledge of the true variance or at least a good upper bound, which is often unknown in practical situations.
This limitation led to the development of the empirical Bernstein inequality.
This data-dependent version estimates the variance from the sample itself, rather than relying on a priori bounds or assumptions.
\begin{proposition}[Empirical Bernstein Inequality,~\cite{maurer2009empirical}]
    \label{prop:empirical-bernstein-inequality}
    Let $X_1, X_2, \ldots, X_n$ be independent random variables with values in $[a,b]$, and let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ be their empirical mean.
    Define the empirical variance as
    \[
        V_n = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2.
    \]
    Then, for any $\delta \in (0,1)$ and $n \geq 2$, with probability at least $1-\delta$,
    \[
        \mathbb{E}[X] \leq \bar{X}_n + \sqrt{\frac{2V_n \ln(1/\delta)}{n}} + \frac{7(b-a)\ln(1/\delta)}{3(n-1)}.
    \]
\end{proposition}
%\subsection{Trash}\label{subsec:trash}
%For the top class, we compute a lower bound on its probability.
%This lower bound represents a value that we can be confident (typically with 95\% or 99\% certainty) is below the true probability.
%By using this lower bound in our calculations, we ensure that we are not overestimating the classifier's confidence in its top prediction.
%Similarly, for all other classes, we compute upper bounds on their probabilities.
%These upper bounds represent values that we can be confident are above the true probabilities for these classes.
%
%The use of these one-sided confidence intervals in the computation of the certified radius leads to a conservative estimate.
%The resulting radius is guaranteed, with high probability, to be no larger than the true radius that would be obtained if we could compute the exact probabilities.
%This conservative approach ensures that the robustness guarantees hold even in the presence of sampling uncertainty.
%
%The specific method used to compute these confidence intervals is typically the Clopper-Pearson interval, also known as the exact binomial confidence interval.
%This method is chosen for its strong guarantees: it ensures that the confidence level is always at least the nominal level, regardless of the true underlying probability.
%While other methods might provide tighter intervals on average, the Clopper-Pearson interval's worst-case guarantees align well with the need for conservative bounds in adversarial robustness certification.
%
%It's important to note that the use of these conservative confidence intervals comes at a cost.
%By always using lower bounds for the top class and upper bounds for other classes, we are effectively reducing the gap between these probabilities.
%This reduction in the probability gap directly translates to a smaller certified radius.
%In other words, our conservative approach trades off some of the potential certified radius to gain statistical confidence in the robustness guarantee.
%
%The impact of this trade-off can be mitigated by increasing the number of Monte Carlo samples.
%As the sample size grows, the confidence intervals narrow, allowing for tighter bounds that more closely approximate the true probabilities.
%However, this comes at the cost of increased computational overhead.
%In practice, the choice of sample size becomes a balance between the desire for tight bounds (and thus larger certified radii) and the computational resources available.
%
%
%In the context of the Cohen et al.
%certified radius, these confidence intervals are applied to the probabilities before they are transformed by the inverse Gaussian CDF. Specifically, if $\underline{p_A}$ is the lower bound on the probability of the top class, and $\overline{p_B}$ is the upper bound on the probability of the runner-up class, the conservative certified radius is computed as:
%
%\[
%    R = \frac{\sigma}{2}(\Phi^{-1}(\underline{p_A}) - \Phi^{-1}(\overline{p_B}))
%\]
%
%This formula encapsulates the conservative nature of the approach: by using $\underline{p_A}$ instead of the point estimate for the top class probability, and $\overline{p_B}$ instead of the point estimate for the runner-up, we ensure that the resulting radius is a valid lower bound on the true robust radius with high probability.
%
%In conclusion, the use of Monte Carlo methods to approximate the output of smoothed classifiers introduces a necessary element of uncertainty into the robustness certification process.
%By employing conservative, one-sided confidence intervals, we manage this uncertainty in a way that preserves the validity of the robustness guarantees, albeit at the cost of potentially smaller certified radii.
%This approach exemplifies the careful balance between theoretical guarantees and practical implementation that characterizes much of adversarial machine learning research.
%
%
