\section{Problem Formulation}\label{sec:problem-formulation}

\subsection{Monte Carlo Simulation}\label{subsec:monte-carlo-simulation}

Both certified radii~\eqref{eq:first-radius} and~\eqref{eq:second-radius} require evaluating the output of the smoothed classifier $\hat{F}$.
The evaluation of a smoothed classifier presents a unique challenge due to the intractable nature of the integral involved in its definition.
To overcome this limitation, the classical approach is to use Monte Carlo methods, which provide a practical and efficient means of approximating the output of the smoothed classifier, capable of scaling to large networks and high-dimensional data.

The Monte Carlo approach relies on the law of large numbers to approximate the true expectation.
The process begins by generating a large number of independent Gaussian noise samples, typically in the order of thousands or tens of thousands.
These noise samples are then added to the input $x$, creating a set of perturbed inputs.
The base classifier $F$ is then evaluated on each of these perturbed inputs, producing a collection of outputs.
The key insight of the Monte Carlo method is that the empirical distribution of these outputs converges to the true distribution of the smoothed classifier's output as the number of samples increases.

The exact procedure differs slightly depending on whether we are dealing with a discrete classifier (outputting class labels directly) or a continuous classifier (outputting logits or probabilities).

\subsubsection{Discrete Case}\label{subsubsec:discrete-case-monte-carlo-simulation}
In the discrete case, our base classifier $F$ outputs one-hot dimensional vectors that we can assimilate to class labels.
To approximate the output of the smoothed classifier $\Hat{F}$, we generate a vector of counts $X = (X_1, \ldots, X_m)$ through a sampling process.
This process begins by generating $n$ independent noise samples from a Gaussian distribution $\mathcal{N}(0, \sigma^2 I)$.
We then evaluate the base classifier on each of these noisy inputs, producing a set of class labels.
Finally, we count the occurrences of each class to form our vector $X$.
The procedure is illustrated in Algorithm~\ref{alg:vector-of-counts}.
\begin{algorithm}[h]
    \DontPrintSemicolon % Removes semicolons at the end of lines
    \KwIn{The base classifier $F$, the number of samples $n$, the standard deviation $\sigma$ and an input $x$}
    \KwOut{The vector of counts $X = (X_1, \ldots, X_m)$}
    Initialize $X = (0, \ldots, 0)$\;
    \For{$i = 1$ \KwTo $n$}{
        Generate noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2 I)$\;
        $\tilde{x}_i = x + \epsilon_i$\;
        $y_i = F(\tilde{x}_i)$\;
        Increment $X_{y_i}$ by 1\;
    }
    \KwRet{$X$}
    \caption{Sampling in the Discrete Case}\label{alg:vector-of-counts}
\end{algorithm}

This vector of counts $X$ can be interpreted as a sample from a multinomial distribution with parameters $n$ and $p = (p_1, \ldots, p_K)$, where $p_k = \mathbb{P}(f(x + \epsilon) = k)$.
The probability vector $p$ is precisely what we aim to approximate, as it represents the output of the smoothed classifier.
To obtain a concrete prediction, we simply select the class with the highest count: $\hat{F}_{out}(x) = \argmax_{k \in \{1, \ldots, m\}} X_k$.
This approach effectively approximates the argmax operation over the class probabilities in the definition of the smoothed classifier.

The law of large numbers ensures that as $n \rightarrow \infty$, the empirical probabilities $X_k / n$ converge to the true probabilities $p_k$, providing a consistent estimator of the smoothed classifier's output.
Moreover, the multinomial nature of $X$ allows us to construct confidence intervals for the true probabilities $p_k$ using methods such as the Clopper-Pearson interval.
These confidence intervals play a crucial role in certifying the robustness of the smoothed classifier, as they allow us to bound the probabilities of the top class and the runner-up with high confidence.

\subsubsection{Continuous Case}\label{subsubsec:continuous-case-monte-carlo-simulation}
In the continuous case, our base classifier $F: \mathbb{R}^d \rightarrow \Delta^{m-1}$ outputs a vector of probabilities for each class.
Here, instead of a vector of counts, we produce a matrix $X \in \mathbb{R}^{n \times m}$ where each row represents the output of the base classifier for a single noisy sample.
The process begins similarly to the discrete case, with the generation of $n$ independent noise samples.
We then evaluate the base classifier on each noisy input, but instead of counting class labels, we store the full output vectors as rows of our matrix $X$.
Algorithm~\ref{alg:probability-matrix} explains the sampling process.
\begin{algorithm}[h]
    \DontPrintSemicolon
    \KwIn{The base classifier $F: \mathbb{R}^d \rightarrow \Delta^{m-1}$, the number of samples $n$, the standard deviation $\sigma$ and an input $x$}
    \KwOut{The grid matrix $X \in \mathbb{R}^{n \times m}$}
    Initialize $X$ as an empty $n \times m$ matrix\;
    \For{$i = 1$ \KwTo $n$}{
        Generate noise $\epsilon_i \sim \mathcal{N}(0, \sigma^2 I)$\;
        $\tilde{x}_i = x + \epsilon_i$\;
        $y_i = F(\tilde{x}_i)$\;
        Set the $i$-th row of $X$ to $y_i$\;
    }
    \KwRet{$X$}
    \caption{Sampling in the Continuous Case}\label{alg:probability-matrix}
\end{algorithm}

Each column of $X$ represents the varying confidence scores for a particular class label across the noisy samples.
The column-wise means $\hat{p}_j = \frac{1}{n} \sum_{i=1}^n X_i^j$ provide an estimate of the expected confidence for each class under the noise distribution, effectively approximating the output of the smoothed classifier.
This approach not only captures the central tendency of the classifier's behavior but also allows us to compute confidence intervals that account for the fluctuations in these scores.
The final prediction is then determined by the class with the highest mean confidence: $\hat{F}_{out}(x) = \argmax_{k \in \{1, \ldots, m\}} \hat{p}_k$.
By considering both the mean and the variability of these confidence scores, we can obtain a more nuanced understanding of the classifier's behavior under noise, which is crucial for both prediction and robustness certification.

In this case, the central limit theorem ensures that $\bar{z}$ converges to a multivariate normal distribution as $n \rightarrow \infty$, centered at the true expected logits.
This allows us to construct confidence intervals for the true expected logits and, consequently, for the probabilities of the smoothed classifier.
The continuous case offers some advantages in terms of statistical efficiency, as it retains more information from each evaluation of the base classifier.
However, it may be more computationally intensive, especially for large $m$, as it requires storing and manipulating the full matrix $Z$ rather than just a vector of counts.

In both the discrete and continuous cases, the accuracy of the Monte Carlo approximation improves with the number of samples $n$, allowing for a trade-off between computational cost and precision.
Larger $n$ provides tighter confidence intervals and more reliable predictions, but at the cost of increased computation time and memory usage.
The choice between the discrete and continuous approaches may depend on the specific base classifier and the computational resources available.
The discrete approach is often simpler to implement and may be more memory-efficient, especially for large $m$.
The continuous approach, while potentially more statistically efficient, requires more storage and computation, particularly when dealing with logits.

In practice, the choice of $n$ and $\sigma$ involves balancing multiple objectives: prediction accuracy, robustness guarantees, computational efficiency, and clean accuracy.
These parameters may need to be tuned based on the specific requirements of the application and the characteristics of the dataset and base classifier.

\subsection{Monte Carlo Approximation and Conservative Confidence Intervals}\label{subsec:monte-carlo-approximation-and-conservative-confidence-intervals}

While the Monte Carlo methods offers a practical means of approximating the output of a smoothed classifier, it introduces an element of randomness into the evaluation process, a factor that must be carefully considered, especially when certifying the robustness of these classifiers, since the certified radius represents a guarantee of the classifier's stability under adversarial perturbations, and any uncertainty in its computation could potentially undermine this guarantee.

The biggest risk induced by the randomness in our Monte Carlo approximation is that our empirical estimate might overstate the true robustness of the classifier.
If we were to naively use our empirical estimates without accounting for this risk, we might end up with an overly optimistic assessment of the classifier's robustness.
Such an overestimation could have serious consequences in security-critical applications, where reliance on an inflated robustness guarantee could lead to vulnerabilities.

To address this risk and maintain the integrity of our robustness guarantees, the use of confidence intervals is key.
Specifically, we employ a one-sided lower confidence bound on our estimate of the certified radius.
In the context of robustness certification, we are primarily concerned with avoiding overestimation of the classifier's capabilities.
An underestimation, while potentially leading to a smaller certified radius, does not compromise the validity of our robustness guarantee.
It merely results in a more conservative certification, which is preferable to an optimistic one that might not hold in practice.

The use of a confidence interval highlights two important trade-offs.
The first trade-off that we touched on in the previous section is the influence of the number of Monte Carlo samples on the width of the confidence interval, and consequently the conservatism of our robustness estimate.
Increasing the number of samples narrows the confidence interval, potentially leading to tighter robustness bounds.
However, this comes at the cost of increased computational overhead.
In practice, the choice of sample size often involves balancing the desire for tight bounds with computational constraints.

Another factor to consider is the choice of confidence level for these intervals.
A higher confidence level (e.g., 99\% vs 95\%) provides stronger statistical guarantees but results in wider intervals and thus smaller certified radii.
Again, this represents a trade-off between the strength of the probabilistic guarantee and the size of the certified radius.

\subsection{Practical Calculation of Confidence Interval}\label{subsec:practical-calculation-of-confidence-interval}

When trying to estimate both certified radii, the heavy-lifting part of the calculation involves estimating the margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$.
In practice, computing this margin exactly as defined can be computationally intensive, especially for classifiers with a large number of classes.
However, a useful simplification can be made without compromising the conservative nature of our robustness estimate.
Instead of considering all classes other than the true class $y$, we can focus solely on the class with the second-highest confidence score, which can be determined empirically.

Formally, if we denote the index of the class with the second-highest score as $j$, we can approximate the margin as:
\begin{align*}
    M(\Hat{F},x) =\Hat{F}(x)_{y} - \Hat{F}(x)_j, \text{ and }M(\Phi^{-1}\circ\Hat{F},x) =\Phi^{-1}(\Hat{F}(x)_{y}) - \Phi^{-1}(\Hat{F}(x)_j).
\end{align*}
This simplification is based on the observation that the class posing the greatest threat to the current classification is the one with the next highest score after the predicted class.
By using only this \("\)runner-up\("\) class in our margin calculation, we ensure that our estimate of the certified radius remains conservative.
This is because the maximum over all other classes will always be greater than or equal to the score of the second-highest class.

The classical statistical approach to estimate the margin is to combine an exact one-sided interval confidence interval with Bonferroni correction and is outlined in Algorithm~\ref{alg:bonferroni}.
The use of exact intervals, rather than approximate ones, is crucial in this context.
Exact intervals guarantee that the true probability is contained within the interval with at least the specified confidence level, regardless of the sample size or the true underlying probability.
This property is particularly important when certifying robustness, as we need to ensure that our guarantee holds with high probability.
Approximate intervals, such as those based on normal approximations, may underestimate the uncertainty for extreme probabilities or small sample sizes, potentially leading to overly optimistic robustness certificates.
The exact interval is then used to get a lower bound to the probability of the predicted class while simultaneously applying an upper bound to the probability of the runner-up class to yield a conservative estimate of the classifier's robustness.

\begin{algorithm}[H]
    \DontPrintSemicolon % Removes semicolons at the end of lines
    \KwIn{A vector of counts or a grid matrix $X$, and a routine \texttt{getConfidenceBound()} that can compute a lower/upper confidence bound at a given confidence level $1-\alpha$ (the arguments of the \texttt{getConfidenceBound()} routine are omitted for conciseness and generalization)}
    \KwOut{The estimated margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$}

    Computer a lower confidence bound $\underline{\Hat{F}(x)}_{y}$ on the probability of the correct class $\Hat{F}(x)_{y}$ using the \texttt{getConfidenceBound()} routine at level $\frac{\alpha}{2}$\;
    Compute an upper confidence bound $\overline{\Hat{F}(x)}_{j}$ on the probability of the second-highest class $\Hat{F}(x)_{y}$ using the \texttt{getConfidenceBound()} routine at level $\frac{\alpha}{2}$\;
    Plug in the lower and upper confidence bounds to the margin $M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$\;
    \KwRet{$M(\Hat{F},x)$ or $M(\Phi^{-1}\circ\Hat{F},x)$}
    \caption{The Bonferroni approach to estimate the margin}\label{alg:bonferroni}
\end{algorithm}

Intuitively, the application of a lower bound to the highest probability effectively diminishes the perceived confidence of the classifier in its primary prediction.
This deliberate underestimation of confidence creates a more stringent condition for establishing robustness guarantees.
It simulates a scenario where the classifier operates under suboptimal conditions, thereby testing its resilience in a more challenging environment than may actually exist.

Conversely, for the runner-up class, the utilization of an upper bound on its probability artificially enhances the perceived strength of the alternative classification.
This overestimation of the competing class's likelihood presents a more formidable challenge to the primary prediction.
The effect is to reduce the apparent margin between the top two classes, thus imposing a more rigorous criterion for demonstrating the classifier's decisiveness.

The confluence of these two conservative estimates – the understatement of the classifier's confidence in its primary prediction and the overstatement of the potential for an alternative classification – engenders a scenario that approaches a worst-case within the bounds of statistical confidence.
This conservative framework ensures that any subsequent robustness guarantees derived from these estimates will remain valid even under these pessimistic assumptions.
Consequently, this approach provides a highly reliable, albeit potentially overly cautious, assessment of the classifier's robustness.

\subsubsection{Discrete Case}\label{subsubsec:discrete-case}
In the discrete case, the Clopper-Pearson interval, also known as the exact binomial confidence interval, provides a conservative estimate of the true probability of an event given a number of observed successes in a fixed number of trials.
In our context, we use this method to construct confidence intervals for the probabilities of the top class and the runner-up class.

To account for the fact that we are estimating multiple probabilities simultaneously, we apply the Bonferroni correction.
This correction adjusts the significance level for each individual confidence interval to ensure that the overall probability of any interval not containing the true value remains below our chosen significance level.

Concretely, let $n_A$ be the number of times the top class $A$ is observed in $n$ Monte Carlo samples, and $n_B$ be the count for the runner-up class $B$.
We construct a lower bound $\underline{p_A}$ for the probability of class $A$ and an upper bound $\overline{p_B}$ for class $B$ using the Clopper-Pearson method:

\begin{equation}
    \underline{p_A} = \text{BetaInv}(\alpha/2, n_A, n - n_A + 1)
\end{equation}
\begin{equation}
    \overline{p_B} = \text{BetaInv}(1-\alpha/2, n_B + 1, n - n_B)
\end{equation}

Here, $\text{BetaInv}$ is the inverse of the cumulative distribution function of the Beta distribution, and $\alpha$ is the chosen significance level (typically 0.001 or smaller).

Consequently, we can use these bounds to compute a conservative estimate of the margin:

\begin{equation}
    \Hat{M}(\Hat{F},x)= \underline{p_A} - \overline{p_B}, \quad\tilde{M}(\Phi^{-1}\circ\Hat{F},x) = \Phi^{-1}(\underline{p_A}) - \Phi^{-1}(\overline{p_B})
\end{equation}

where $\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function.

This conservative margin estimate can then be used to compute the certified radius, ensuring that our robustness guarantee holds with high probability.
The use of these statistical techniques allows us to make strong claims about the robustness of our smoothed classifier, even when working with finite samples in a probabilistic setting.
By consistently underestimating the true margin, we maintain the validity of our robustness certificates at the cost of potentially being overly conservative in some cases.

\subsubsection{Continuous Case}\label{subsubsec:continuous-case}
In the continuous case, to obtain a lower or an upper bound on the mean of a class probability, the corresponding column in the prediction matrix $X$ serves as a sample we can use to estimate the mean with an exact confidence interval.
The classical approach to obtaining confidence intervals involves inverting concentration inequalities which describe how a random variable deviates from its expected value.
These inequalities provide upper bounds on the probability that a random variable differs from its mean by more than a certain amount.
Common examples include Markov's inequality, Chebyshev's inequality, and Hoeffding's inequality.
To derive a confidence interval, we simply invert these concentration inequalities.

This inversion essentially flips the perspective: instead of bounding the probability of deviation, we bound the range of possible values for the parameter of interest.
The key idea is to express the probability statement in terms of the unknown parameter rather than the random variable.
This inversion process typically involves algebraic manipulation of the inequality.
We start with the concentration inequality, which gives us a probabilistic bound on the deviation of our estimator from the true parameter value.
By rearranging this inequality, we can express it in terms of the parameter itself, creating a range of values that likely contains the true parameter with a specified level of confidence.

The resulting confidence interval provides a range of plausible values for the unknown parameter, along with a measure of the uncertainty associated with the estimate.
The width of this interval is influenced by factors such as the sample size, the variability in the data, and the desired level of confidence.

In randomized smoothing, the state-of-the-art approach uses a variance-adaptive concentration inequality to obtain confidence bounds.
Other variance-agnostic approaches have limitations, particularly in scenarios with low variance which are the scenarios that we are interested in, since well-trained classifiers (like neural networks trained using Gaussian data augmentation or adversarial training) have very low variance in their outputs for similar inputs.
For example, Hoeffding's inequality assumes the worst-case scenario for variance, which is $1/4$ for a bounded random variable in $[0,1]$.
This assumption can lead to overly wide confidence intervals when the actual variance is much smaller than this upper bound.

Unlike Hoeffding's, Bernstein's inequality takes into account the actual variance of the random variables.
By incorporating this information, Bernstein's inequality can provide tighter bounds, especially in low-variance scenarios relative to their range.
However, the standard Bernstein inequality still requires knowledge of the true variance or at least a good upper bound, which is often unknown in practical situations.
This limitation led to the development of the empirical Bernstein inequality.
This data-dependent version estimates the variance from the sample itself, rather than relying on a priori bounds or assumptions.
\begin{proposition}[Empirical Bernstein Inequality]
    Let $X_1, X_2, \ldots, X_n$ be independent random variables with values in $[a,b]$, and let $\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$ be their empirical mean.
    Define the empirical variance as
    \[
        V_n = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2.
    \]
    Then, for any $\delta \in (0,1)$ and $n \geq 2$, with probability at least $1-\delta$,
    \[
        \mathbb{E}[X] \leq \bar{X}_n + \sqrt{\frac{2V_n \ln(1/\delta)}{n}} + \frac{7(b-a)\ln(1/\delta)}{3(n-1)}.
    \]
\end{proposition}
%\subsection{Trash}\label{subsec:trash}
%For the top class, we compute a lower bound on its probability.
%This lower bound represents a value that we can be confident (typically with 95\% or 99\% certainty) is below the true probability.
%By using this lower bound in our calculations, we ensure that we are not overestimating the classifier's confidence in its top prediction.
%Similarly, for all other classes, we compute upper bounds on their probabilities.
%These upper bounds represent values that we can be confident are above the true probabilities for these classes.
%
%The use of these one-sided confidence intervals in the computation of the certified radius leads to a conservative estimate.
%The resulting radius is guaranteed, with high probability, to be no larger than the true radius that would be obtained if we could compute the exact probabilities.
%This conservative approach ensures that the robustness guarantees hold even in the presence of sampling uncertainty.
%
%The specific method used to compute these confidence intervals is typically the Clopper-Pearson interval, also known as the exact binomial confidence interval.
%This method is chosen for its strong guarantees: it ensures that the confidence level is always at least the nominal level, regardless of the true underlying probability.
%While other methods might provide tighter intervals on average, the Clopper-Pearson interval's worst-case guarantees align well with the need for conservative bounds in adversarial robustness certification.
%
%It's important to note that the use of these conservative confidence intervals comes at a cost.
%By always using lower bounds for the top class and upper bounds for other classes, we are effectively reducing the gap between these probabilities.
%This reduction in the probability gap directly translates to a smaller certified radius.
%In other words, our conservative approach trades off some of the potential certified radius to gain statistical confidence in the robustness guarantee.
%
%The impact of this trade-off can be mitigated by increasing the number of Monte Carlo samples.
%As the sample size grows, the confidence intervals narrow, allowing for tighter bounds that more closely approximate the true probabilities.
%However, this comes at the cost of increased computational overhead.
%In practice, the choice of sample size becomes a balance between the desire for tight bounds (and thus larger certified radii) and the computational resources available.
%
%
%In the context of the Cohen et al.
%certified radius, these confidence intervals are applied to the probabilities before they are transformed by the inverse Gaussian CDF. Specifically, if $\underline{p_A}$ is the lower bound on the probability of the top class, and $\overline{p_B}$ is the upper bound on the probability of the runner-up class, the conservative certified radius is computed as:
%
%\[
%    R = \frac{\sigma}{2}(\Phi^{-1}(\underline{p_A}) - \Phi^{-1}(\overline{p_B}))
%\]
%
%This formula encapsulates the conservative nature of the approach: by using $\underline{p_A}$ instead of the point estimate for the top class probability, and $\overline{p_B}$ instead of the point estimate for the runner-up, we ensure that the resulting radius is a valid lower bound on the true robust radius with high probability.
%
%In conclusion, the use of Monte Carlo methods to approximate the output of smoothed classifiers introduces a necessary element of uncertainty into the robustness certification process.
%By employing conservative, one-sided confidence intervals, we manage this uncertainty in a way that preserves the validity of the robustness guarantees, albeit at the cost of potentially smaller certified radii.
%This approach exemplifies the careful balance between theoretical guarantees and practical implementation that characterizes much of adversarial machine learning research.
%
%
